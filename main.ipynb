{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imort Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import keras.callbacks as cb\n",
    "from keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from CustomMetricsLosses import *\n",
    "import argparse\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from scipy.interpolate import Rbf,griddata,RegularGridInterpolator\n",
    "from scipy.signal import resample\n",
    "from Model_Unet import *\n",
    "import GPUtil\n",
    "import os\n",
    "import easydict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea sessione tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4011113144664175307\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7769907552\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4301698976223770021\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU selected: 0\n"
     ]
    }
   ],
   "source": [
    "GPU = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "print('GPU selected:', str(GPU))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import clear_session\n",
    "\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizializza variabili globali\n",
    "num_x_points = 64\n",
    "num_y_points = 16\n",
    "num_freqs = 512\n",
    "num_dimensions = 3\n",
    "x = np.arange(0,num_x_points,1).tolist() # x-axis\n",
    "y = np.arange(0,num_y_points,1).tolist() # y-axis\n",
    "\n",
    "def normalize(in_content):\n",
    "    in_content = np.abs(in_content)\n",
    "    max_el = in_content.max()\n",
    "    in_content_norm = in_content/max_el\n",
    "    return in_content_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Part(prepare&split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTestSet(init,end,x_down_factor,y_down_factor,num_freqs,tens_x_file,downsampling):\n",
    "    zero_lines_idxs = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    x = np.arange(0,num_x_points,1).tolist()\n",
    "    counter_array = np.arange(init,end+1,1)\n",
    "    datapath = './dataset/DatasetFiles/Dataset_xyf/Dataset_xyf_'\n",
    "\n",
    "    for count in counter_array:\n",
    "        with open(datapath+str(count), 'rb') as data:\n",
    "            dati = pickle.load(data)\n",
    "        tens_init_freq = np.arange(0,1024,num_freqs)\n",
    "\n",
    "        print('')\n",
    "        print('Preparing '+datapath+str(count))\n",
    "        print('')\n",
    "\n",
    "        list_tens = sample(np.arange(0,len(dati),1).tolist(),k=tens_x_file)\n",
    "\n",
    "        for step,tens_idx in enumerate(list_tens):\n",
    "\n",
    "            for i,init_freq in enumerate(tens_init_freq):\n",
    "\n",
    "                if init_freq>1024-num_freqs:\n",
    "                    break\n",
    "                end_freq = init_freq+num_freqs\n",
    "                freq = np.arange(init_freq,end_freq,1)\n",
    "\n",
    "                target_tens = np.array(dati[tens_idx][5][:,:,init_freq:end_freq])\n",
    "                input_tens = np.array(dati[tens_idx][5][:,:,init_freq:end_freq])\n",
    "\n",
    "                if downsampling=='regular':\n",
    "                    x_sampled_list = x[::int(1/x_down_factor)]\n",
    "                    y_sampled_list = y[::int(1/y_down_factor)]\n",
    "                else:\n",
    "                    x_sampled_list = sample(x,k=int(num_x_points*(x_down_factor)))\n",
    "                    x_sampled_list.sort()\n",
    "                    y_sampled_list = sample(y,k=int(num_y_points*(y_down_factor)))\n",
    "                    y_sampled_list.sort()\n",
    "\n",
    "                sampled_list = x_sampled_list+y_sampled_list\n",
    "\n",
    "                if downsampling=='random':\n",
    "                    i=0\n",
    "                    for xx in x:\n",
    "                        if i==len(x_sampled_list):\n",
    "                            input_tens[:,xx:num_x_points,:]=np.zeros((num_y_points,num_x_points-xx,num_freqs))\n",
    "                            break\n",
    "                        elif xx!=sampled_list[i]:\n",
    "                            input_tens[:,xx,:]=np.zeros((num_y_points,num_freqs))\n",
    "                        else:\n",
    "                            i=i+1\n",
    "                    for yy in y:\n",
    "                        if i==len(sampled_list):\n",
    "                            input_tens[yy:num_y_points,:,:]=np.zeros((num_y_points-yy,num_x_points,num_freqs))\n",
    "                            break\n",
    "                        elif yy!=sampled_list[i]:\n",
    "                            input_tens[yy,:,:]=np.zeros((num_x_points,num_freqs))\n",
    "                        else:\n",
    "                            i=i+1\n",
    "\n",
    "                elif downsampling=='regular':\n",
    "                    for xx in x:\n",
    "                        if xx%int(1/x_down_factor)!=0:\n",
    "                            input_tens[:,xx,:]=np.zeros((num_y_points,num_freqs))\n",
    "                    for yy in y:\n",
    "                        if yy%int(1/y_down_factor)!=0:\n",
    "                            input_tens[yy,:,:]=np.zeros((num_x_points,num_freqs))\n",
    "\n",
    "                zero_lines_idxs.append(sampled_list)\n",
    "                X_test.append(normalize(input_tens))\n",
    "                Y_test.append(normalize(target_tens))\n",
    "\n",
    "                '''plt.subplot(121), plt.title('Input xy image')\n",
    "                plt.imshow(input_tens[:,:,120], cmap='bone', aspect='auto'), plt.colorbar()\n",
    "                plt.xlabel('X [m]'), plt.ylabel('Y [m]')\n",
    "                #plt.grid(None)\n",
    "                plt.subplot(122), plt.title('Target xy image')\n",
    "                plt.imshow(target_tens[:,:,120], cmap='bone', aspect='auto'), plt.colorbar()\n",
    "                plt.xlabel('X [m]'), plt.ylabel('Y [m]')\n",
    "                #plt.grid(None)\n",
    "                plt.show()'''\n",
    "\n",
    "\n",
    "    print('')\n",
    "    print('Test set composed by --> '+str(len(X_test))+' tensors')\n",
    "    print('')\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    ### ADD CHANNEL DIMENSION\n",
    "    X_test = X_test.reshape(len(X_test),num_y_points,num_x_points,num_freqs,1)\n",
    "    Y_test = Y_test.reshape(len(X_test),num_y_points,num_x_points,num_freqs,1)\n",
    "    return X_test,Y_test, zero_lines_idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = easydict.EasyDict({\n",
    "\"init\": 1,\n",
    "\"end\": 1,\n",
    "\"lr\": 0.0004,\n",
    "\"tens_x_file\": 1,\n",
    "\"downsampling\": 'regular',\n",
    "\"method\": 'interp',\n",
    "\"interp_func\": 'linear'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "if args.downsampling=='regular':\n",
    "    x_down_factors = np.array([0.25, 0.5, 0.5])\n",
    "    y_down_factors = np.array([0.5, 0.5, 1])\n",
    "else:\n",
    "    x_down_factors = np.array([0.2,0.4,0.5,0.5,0.5,0.6,0.7,0.8,0.9])\n",
    "    y_down_factors = np.array([0.5,0.5,0.6,0.8,1,1,1,1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile model (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing ./dataset/DatasetFiles/Dataset_xyf/Dataset_xyf_1\n",
      "\n",
      "\n",
      "Test set composed by --> 2 tensors\n",
      "\n",
      "\n",
      "X_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Y_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Testing Linear Interpolation on regular down tensors with 12.5 %% of original data\n",
      "\n",
      "Calculating Interp NMSE and NCC for reconstructions\n",
      "\n",
      "Preparing ./dataset/DatasetFiles/Dataset_xyf/Dataset_xyf_1\n",
      "\n",
      "\n",
      "Test set composed by --> 2 tensors\n",
      "\n",
      "\n",
      "X_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Y_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Testing Linear Interpolation on regular down tensors with 25.0 %% of original data\n",
      "\n",
      "Calculating Interp NMSE and NCC for reconstructions\n",
      "\n",
      "Preparing ./dataset/DatasetFiles/Dataset_xyf/Dataset_xyf_1\n",
      "\n",
      "\n",
      "Test set composed by --> 2 tensors\n",
      "\n",
      "\n",
      "X_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Y_test dimensions: (2, 16, 64, 512, 1)\n",
      "\n",
      "Testing Linear Interpolation on regular down tensors with 50.0 %% of original data\n",
      "\n",
      "Calculating Interp NMSE and NCC for reconstructions\n"
     ]
    }
   ],
   "source": [
    "for count in range(len(x_down_factors)):\n",
    "\n",
    "    x_down_factor = x_down_factors[count]\n",
    "    y_down_factor = y_down_factors[count]\n",
    "\n",
    "    X_test,Y_test,zero_row_idxs = prepareTestSet(args.init,args.end,x_down_factor,y_down_factor,num_freqs,args.tens_x_file,args.downsampling)\n",
    "\n",
    "    print('')\n",
    "    print('X_test dimensions: '+ str(np.shape(X_test)))\n",
    "    print('')\n",
    "    print('Y_test dimensions: '+ str(np.shape(Y_test)))\n",
    "\n",
    "    list_metrics_interp = []\n",
    "    list_plots_interp = []\n",
    "    freq = np.arange(0,num_freqs,1).tolist()  #frequency axis\n",
    "    grid_y, grid_x, grid_freq = np.mgrid[ 0:num_y_points:1, 0:num_x_points:1, 0:num_freqs:1]\n",
    "    x_ds = np.linspace(0,num_x_points,int(num_x_points*(x_down_factor))).tolist()\n",
    "    y_ds = np.linspace(0,num_y_points,int(num_y_points*(y_down_factor))).tolist()\n",
    "    ds_matrix_points = len(x_ds)*len(y_ds)*len(freq)\n",
    "\n",
    "    if args.method=='interp':\n",
    "\n",
    "        for idx in range(len(Y_test)):\n",
    "            #print('Tensor n°: '+str(idx))\n",
    "\n",
    "            target_tens = Y_test[idx][:,:,:,0]\n",
    "            down = X_test[idx][:,:,:,0]\n",
    "\n",
    "            x_ds_tens = np.zeros((num_y_points,int(num_x_points*x_down_factor),num_freqs))\n",
    "\n",
    "            zero_row_idx=0\n",
    "            count=0\n",
    "\n",
    "            for j in x:\n",
    "                if j==zero_row_idxs[idx][zero_row_idx]:\n",
    "                    x_ds_tens[:,count,:] = down[:,j,:]\n",
    "                    count=count+1\n",
    "                    zero_row_idx=zero_row_idx+1\n",
    "                if zero_row_idx==int(num_x_points*(x_down_factor)):\n",
    "                    break\n",
    "\n",
    "            count=0\n",
    "            ds_tens = np.zeros((int(num_y_points*y_down_factor),int(num_x_points*x_down_factor),num_freqs))\n",
    "\n",
    "            for i in y:\n",
    "                if i==zero_row_idxs[idx][zero_row_idx]:\n",
    "                    ds_tens[count,:,:] = x_ds_tens[i,:,:]\n",
    "                    count=count+1\n",
    "                    zero_row_idx=zero_row_idx+1\n",
    "                if zero_row_idx==len(zero_row_idxs[idx]):\n",
    "                    break\n",
    "\n",
    "            if args.interp_func=='Fourier' and idx==0:\n",
    "                # RESAMPLE --> FOURIER-BASED INTERPOLATOR\n",
    "                if idx==0:\n",
    "                    print('')\n",
    "                    print('Testing Fourier-based Interpolation on '+args.downsampling+' down tensors with '+str(x_down_factor*y_down_factor*100)+' %% of original data')\n",
    "                    print('')\n",
    "                interp_y= resample(ds_tens,num_y_points,axis=0)\n",
    "                interp_tens= resample(interp_y,num_x_points,axis=1)\n",
    "\n",
    "            elif args.interp_func=='NN' and idx==0:\n",
    "                # NN INTERPOLATOR\n",
    "                if idx==0:\n",
    "                    print('')\n",
    "                    print('Testing Nearest-Neighbour Interpolation on '+args.downsampling+' down tensors with '+str(x_down_factor*y_down_factor*100)+' %% of original data')\n",
    "                    print('')\n",
    "                num_points = len(x_ds)*len(y_ds)*len(freq)\n",
    "                points = np.zeros((num_points,num_dimensions))\n",
    "                values = np.zeros((num_points))\n",
    "                count = 0\n",
    "\n",
    "                for i in range(len(y_ds)):\n",
    "                    for j in range (len(x_ds)):\n",
    "                        for k in range(len(freq)):\n",
    "                            points[count,:] = (y_ds[i], x_ds[j], freq[k])\n",
    "                            values[count] = ds_tens[i,j,k]\n",
    "                            count = count+1\n",
    "\n",
    "                interp_tens = griddata(points, values, (grid_y, grid_x, grid_freq), method='nearest')\n",
    "\n",
    "            elif args.interp_func=='linear':\n",
    "                # LINEAR INTERPOLATOR\n",
    "                if idx==0:\n",
    "                    print('')\n",
    "                    print('Testing Linear Interpolation on '+args.downsampling+' down tensors with '+str(x_down_factor*y_down_factor*100)+' %% of original data')\n",
    "                    print('')\n",
    "                interp = RegularGridInterpolator((y_ds, x_ds, freq), ds_tens, method='linear')\n",
    "                interp_tens = interp((grid_y, grid_x, grid_freq))\n",
    "\n",
    "            elif args.interp_func=='Rbf':\n",
    "                # Rbf INTERPOLATOR\n",
    "                if idx==0:\n",
    "                    print('')\n",
    "                    print('Testing Rbf Interpolation on '+args.downsampling+' down tensors with '+str(x_down_factor*y_down_factor*100)+' %% of original data')\n",
    "                    print('')\n",
    "                values = np.zeros((ds_matrix_points))\n",
    "                rows = np.zeros((ds_matrix_points))\n",
    "                columns = np.zeros((ds_matrix_points))\n",
    "                freqs_values = np.zeros((ds_matrix_points))\n",
    "                count=0\n",
    "\n",
    "                for i in range(int(num_y_points*y_down_factor)):\n",
    "                    for j in range(int(num_x_points*x_down_factor)):\n",
    "                        for k in range(args.num_freqs):\n",
    "                            values[count] = ds_tens[i,j,k]\n",
    "                            rows[count] = i\n",
    "                            columns[count] = j\n",
    "                            freqs_values[count] = k\n",
    "                            count = count+1\n",
    "\n",
    "                rbf = Rbf(freqs_values, columns, rows, values, function='cubic')  # radial basis function interpolator instance\n",
    "                XI, YI, ZI = np.meshgrid(freq, x*x_down_factor,y*y_down_factor)\n",
    "                interp_tens = rbf(XI, YI, ZI)\n",
    "\n",
    "            else:\n",
    "                print('ERRORE! CONTROLLA PARAMETRO interp_func')\n",
    "                exit()\n",
    "\n",
    "            nmse_interp = nmse(target_tens,interp_tens)\n",
    "            ncc_interp = NCC(target_tens,interp_tens)\n",
    "            list_metrics_interp.append((nmse_interp,ncc_interp))\n",
    "\n",
    "        print(\"Calculating Interp NMSE and NCC for reconstructions\")\n",
    "\n",
    "        if args.downsampling=='regular':\n",
    "            with open('./Metrics/3D/Paper/Interps/Regular/metrics_3D_'+args.interp_func+'_interp_downtest'+str(x_down_factor*y_down_factor*100)+'%%data','wb') as output:\n",
    "                pickle.dump(list_metrics_interp,output)\n",
    "        else:\n",
    "            with open('./Metrics/3D/Paper/Interps/Random/metrics_3D_'+args.interp_func+'_interp_downtest'+str(x_down_factor*y_down_factor*100)+'%%data','wb') as output:\n",
    "                pickle.dump(list_metrics_interp,output)\n",
    "\n",
    "    elif args.method=='Unet':\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "        down_factors = np.array([2,4,8])\n",
    "        #down_factors = np.array([4,8])\n",
    "\n",
    "        for step in range(len(down_factors)):\n",
    "\n",
    "            print('')\n",
    "            print('Testing 3D U-net trained on '+args.downsampling+' down '+str(down_factors[step])+' tensors, on tensors with '+str(x_down_factor*y_down_factor*100)+' %% of original data')\n",
    "            print('')\n",
    "\n",
    "            if args.downsampling=='regular':\n",
    "                if step==0:\n",
    "                    uNet = load_model('./ModelCheckpoint/3D/down_x/Regular/super_res_3D_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse_3D(batch_size=1, num_freqs=num_freqs),'NMSE': NMSE, 'ncc': ncc})\n",
    "                else:\n",
    "                    uNet = load_model('./ModelCheckpoint/3D/down_xy/Regular/super_res_3D_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse_3D(batch_size=1, num_freqs=num_freqs),'NMSE': NMSE, 'ncc': ncc})\n",
    "\n",
    "                uNet.compile(loss=mask_mse_3D(batch_size=1, num_freqs=args.num_freqs), optimizer=opt, metrics=[NMSE, ncc])\n",
    "\n",
    "            elif args.downsampling=='random':\n",
    "                if step==0:\n",
    "                    uNet = load_model('./ModelCheckpoint/3D/down_x/Random/super_res_3D_random_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'mask_mse_3D': mask_mse_3D,'NMSE': NMSE, 'ncc': ncc})\n",
    "                    uNet.compile(loss=mask_mse_3D, optimizer=opt, metrics=[NMSE, ncc])\n",
    "                else:\n",
    "                    uNet = load_model('./ModelCheckpoint/3D/down_xy/Random/super_res_3D_random_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse_3D(batch_size=1, num_freqs=num_freqs),'NMSE': NMSE, 'ncc': ncc})\n",
    "                    uNet.compile(loss=mask_mse_3D(batch_size=1, num_freqs=num_freqs), optimizer=opt, metrics=[NMSE, ncc])\n",
    "\n",
    "            score = uNet.evaluate(X_test, Y_test, verbose=1, batch_size=1)\n",
    "            probs = uNet.predict(X_test, verbose=1, batch_size=1)\n",
    "\n",
    "            print(\"Calculating U-net NMSE and NCC for predictions\")\n",
    "\n",
    "            list_metrics_Unet = []\n",
    "            list_plots_Unet = []\n",
    "\n",
    "            for idx in range(len(Y_test)):\n",
    "                down = X_test[idx][:,:,:,0]\n",
    "                ground_truth = Y_test[idx][:,:,:,0]\n",
    "                prediction = probs[idx][:,:,:,0]\n",
    "\n",
    "                nmse_Unet = nmse(ground_truth,prediction)\n",
    "                ncc_Unet = NCC(ground_truth,prediction)\n",
    "                list_metrics_Unet.append((nmse_Unet,ncc_Unet))\n",
    "\n",
    "            if args.downsampling=='regular':\n",
    "                with open('./Metrics/3D/Paper/Unets/Regular/metrics_3D_Unet_downtrain'+str(down_factors[step])+'_downtest'+str(x_down_factor*y_down_factor*100)+'%%data','wb') as output:\n",
    "                 pickle.dump(list_metrics_Unet,output)\n",
    "            else:\n",
    "                with open('./Metrics/3D/Paper/Unets/Random/metrics_3D_Unet_downtrain'+str(down_factors[step])+'_downtest'+str(x_down_factor*y_down_factor*100)+'%%data','wb') as output:\n",
    "                 pickle.dump(list_metrics_Unet,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the U-net model (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate custom matrics and save them using pickle (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
