{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imort Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import keras.callbacks as cb\n",
    "from keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from CustomMetricsLosses import *\n",
    "import argparse\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from scipy.interpolate import Rbf,interp2d\n",
    "from scipy.signal import resample\n",
    "from Model_Unet import *\n",
    "import GPUtil\n",
    "import os\n",
    "\n",
    "GPU = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "print('GPU selected:', str(GPU))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea sessione tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3854324577203318581\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7769907552\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18446694796218544693\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "config=tf.compat.v1.ConfigProto()\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(in_content):\n",
    "    in_content = np.abs(in_content)\n",
    "    max_el = in_content.max()\n",
    "    in_content_norm = in_content/max_el\n",
    "    return in_content_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Part(prepare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTestSet(init,end,x_down_factor,num_x_points,num_freqs,imgs_x_file,downsampling):\n",
    "\n",
    "    zero_lines_idxs = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    x = np.arange(0,num_x_points,1).tolist()\n",
    "\n",
    "    counter_array = np.arange(init,end+1,1)\n",
    "\n",
    "    datapath = '../dataset/Dataset_complex/dataset_complex_'\n",
    "\n",
    "    for count in counter_array:\n",
    "        with open(datapath+str(count), 'rb') as data:\n",
    "            dati = pickle.load(data)\n",
    "\n",
    "        print('')\n",
    "        print('Preparing '+datapath+str(count))\n",
    "        print('')\n",
    "\n",
    "        list_images = sample(np.arange(0,len(dati),1).tolist(),k=imgs_x_file)\n",
    "\n",
    "        for step,img_idx in enumerate(list_images):\n",
    "            target_img = np.array(dati[img_idx][6])\n",
    "            input_img = np.zeros((num_x_points,num_freqs))\n",
    "\n",
    "            if downsampling=='regular':\n",
    "                sampled_list = x[::int(1/x_down_factor)]\n",
    "            else:\n",
    "                sampled_list = sample(x,k=int(num_x_points*x_down_factor))\n",
    "                sampled_list.sort()\n",
    "\n",
    "            i=0\n",
    "            for idx in x:\n",
    "                if i==int(num_x_points*(x_down_factor)):\n",
    "                    break\n",
    "                elif idx==sampled_list[i]:\n",
    "                    input_img[idx,:]=target_img[idx,:]\n",
    "                    i=i+1                    \n",
    "\n",
    "            if np.mean(target_img**2)>1e-10:\n",
    "                zero_lines_idxs.append(sampled_list)\n",
    "                X_test.append(normalize(input_img))\n",
    "                Y_test.append(normalize(target_img))\n",
    "\n",
    "                '''plt.subplot(121), plt.title('Input xy image')\n",
    "                plt.imshow(normalize(input_img), cmap='bone', aspect='auto'), plt.colorbar()\n",
    "                plt.xlabel('X [m]'), plt.ylabel('Y [m]')\n",
    "                #plt.grid(None)\n",
    "                plt.subplot(122), plt.title('Target xy image')\n",
    "                plt.imshow(normalize(target_img), cmap='bone', aspect='auto'), plt.colorbar()\n",
    "                plt.xlabel('X [m]'), plt.ylabel('Y [m]')\n",
    "                #plt.grid(None)\n",
    "                plt.show()'''\n",
    "\n",
    "    print('')\n",
    "    print('Test set composed by --> '+str(len(X_test))+' images')\n",
    "    print('')\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    ### ADD CHANNEL DIMENSION\n",
    "    X_test = X_test.reshape(len(X_test),num_x_points,num_freqs,1)\n",
    "    Y_test = Y_test.reshape(len(X_test),num_x_points,num_freqs,1)\n",
    "    return X_test,Y_test, zero_lines_idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial part (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing ../dataset/Dataset_complex/dataset_complex_1\n",
      "\n",
      "Percentage: 10%\n",
      "Percentage: 20%\n",
      "Percentage: 30%\n",
      "Percentage: 40%\n",
      "Percentage: 50%\n",
      "Percentage: 60%\n",
      "Percentage: 70%\n",
      "Percentage: 80%\n",
      "Percentage: 90%\n",
      "Percentage: 100%\n",
      "\n",
      "Dataset ready to be splitted --> 1632 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    ".py args：\n",
    "\n",
    "\"\"\"\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--init',type=int,required=False,default=10)\n",
    "# parser.add_argument('--end',type=int,required=False,default=10)\n",
    "# parser.add_argument('--num_x_points',type=int,required=False,default=64)\n",
    "# parser.add_argument('--num_freqs',type=int,required=False,default=1024)\n",
    "# parser.add_argument('--lr',type=float,required=False,default=0.0004)\n",
    "# parser.add_argument('--imgs_x_file',type=int,required=False,default=30)\n",
    "# parser.add_argument('--interp_func',type=str,required=False,default='Bicubic')\n",
    "# parser.add_argument('--downsampling',type=str,required=False,default='percent') #regular\n",
    "# parser.add_argument('--method',type=str,required=False,default='Unet')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ".ipynb args：\n",
    "\n",
    "\"\"\"\n",
    "args = easydict.EasyDict({\n",
    "    \"init\": 10,\n",
    "    \"end\": 10,\n",
    "    \"num_x_points\": 64,\n",
    "    \"num_freqs\": 1024,\n",
    "    \"lr\": 0.0004,\n",
    "    \"imgs_x_file\": 30,\n",
    "    \"interp_func\": 'Bicubic',\n",
    "    \"downsampling\": 'percent', #regular\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main part (main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.downsampling=='regular':\n",
    "        x_down_factors = [0.125,0.25,0.5]\n",
    "    else:\n",
    "        x_down_factors = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "    for count in range(len(x_down_factors)):\n",
    "\n",
    "        x_down_factor = x_down_factors[count]\n",
    "\n",
    "        X_test,Y_test,zero_row_idxs = prepareTestSet(args.init,args.end,x_down_factor,args.num_x_points,args.num_freqs,args.imgs_x_file,args.downsampling)\n",
    "\n",
    "        print('')\n",
    "        print('X_test dimensions: '+ str(np.shape(X_test)))\n",
    "        print('')\n",
    "        print('Y_test dimensions: '+ str(np.shape(Y_test)))\n",
    "\n",
    "        num_x_points = args.num_x_points\n",
    "        num_freqs = args.num_freqs\n",
    "        x = np.arange(0,num_x_points,1).tolist() # x-axis\n",
    "        freq = np.arange(0,num_freqs,1).tolist()  #frequency axis\n",
    "        ds_matrix_points = int(num_x_points*x_down_factor)*num_freqs\n",
    "\n",
    "        if args.method=='interp':\n",
    "\n",
    "            list_metrics_interp = []\n",
    "            list_plots_interp = []\n",
    "\n",
    "            for idx in range(len(Y_test)):\n",
    "                #print('Image n° '+str(idx))\n",
    "                target_img = Y_test[idx][:,:,0]\n",
    "                ds_img = np.zeros((int(num_x_points*x_down_factor),num_freqs))\n",
    "                zero_row_idx=0\n",
    "                count=0\n",
    "\n",
    "                x_ds = np.linspace(0,num_x_points,int(num_x_points*x_down_factor)).tolist()\n",
    "\n",
    "                for i in x:\n",
    "                    if i==zero_row_idxs[idx][zero_row_idx]:\n",
    "                        ds_img[count,:] = target_img[i,:]\n",
    "                        count=count+1\n",
    "                        zero_row_idx=zero_row_idx+1\n",
    "                    if zero_row_idx==int(num_x_points*(x_down_factor)):\n",
    "                        break\n",
    "\n",
    "                if args.interp_func=='Rbf':\n",
    "                    # Rbf Interpolator\n",
    "                    if idx==0:\n",
    "                        print('')\n",
    "                        print('Testing Rbf Interpolation on '+args.downsampling+' down images with '+str(x_down_factor*100)+' %% of original data')\n",
    "                        print('')\n",
    "                    values = np.zeros((ds_matrix_points))\n",
    "                    rows = np.zeros((ds_matrix_points))\n",
    "                    columns = np.zeros((ds_matrix_points))\n",
    "                    count=0\n",
    "\n",
    "                    for i in range(int(num_x_points*x_down_factor)):\n",
    "                        for j in range(num_freqs):\n",
    "                            values[count] = ds_img[i,j]\n",
    "                            rows[count] = i\n",
    "                            columns[count] = j\n",
    "                            count = count+1\n",
    "\n",
    "                    rbf = Rbf(columns, rows, values, function='cubic')  # radial basis function interpolator instance\n",
    "                    XI, YI = np.meshgrid(freq, x/args.down_factor)\n",
    "                    interp_img = rbf(XI, YI)\n",
    "\n",
    "                elif args.interp_func=='Fourier':\n",
    "                    # ResSample Interpolator:\n",
    "                    if idx==0:\n",
    "                        print('')\n",
    "                        print('Testing Fourier-based Interpolation on '+args.downsampling+' down images with '+str(x_down_factor*100)+' %% of original data')\n",
    "                        print('')\n",
    "                    interp_img= resample(ds_img,num_x_points,axis=0)\n",
    "\n",
    "                elif args.interp_func=='Bicubic':\n",
    "                    # Bicubic Interpolator:\n",
    "                    if idx==0:\n",
    "                        print('')\n",
    "                        print('Testing Bicubic Interpolation on '+args.downsampling+' down images with '+str(x_down_factor*100)+' %% of original data')\n",
    "                        print('')\n",
    "                    interp = interp2d(freq, x_ds, ds_img, kind='cubic')\n",
    "                    interp_img = interp(freq,x)\n",
    "\n",
    "                else:\n",
    "                    print('ERRORE! CONTROLLA PARAMETRO interp_func')\n",
    "                    exit()\n",
    "\n",
    "                nmse_interp = nmse(target_img,interp_img)\n",
    "                ncc_interp = NCC(target_img,interp_img)\n",
    "                list_metrics_interp.append((nmse_interp,ncc_interp))\n",
    "                list_plots_interp.append((target_img,interp_img))\n",
    "\n",
    "            print(\"Calculating Interp NMSE and NCC for reconstructions\")\n",
    "\n",
    "            if args.downsampling=='regular':\n",
    "                with open('./Metrics/2D/xf/64/Paper/Interps/Regular/metrics_2D_'+args.interp_func+'_interp_downtest'+str(x_down_factor*100)+'%%data','wb') as output:\n",
    "                    pickle.dump(list_metrics_interp,output)\n",
    "            elif args.downsampling=='random':\n",
    "                with open('./Metrics/2D/xf/64/Paper/Interps/Random/metrics_2D_'+args.interp_func+'_interp_downtest'+str(x_down_factor*100)+'%%data','wb') as output:\n",
    "                    pickle.dump(list_metrics_interp,output)\n",
    "            else:\n",
    "                with open('../Metrics/Metrics_behaviour_'+args.interp_func+'_interp_downtest'+str(x_down_factor*100)+'%%data','wb') as output:\n",
    "                    pickle.dump(list_metrics_interp,output)\n",
    "                \n",
    "\n",
    "        elif args.method=='Unet':\n",
    "\n",
    "            opt = keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "            down_factors = np.array([2,4,8])\n",
    "\n",
    "            for step in range(len(down_factors)):\n",
    "                print('')\n",
    "                print('Testing 2D U-net trained on '+args.downsampling+' down '+str(down_factors[step])+' images, on images with '+str(x_down_factor*100)+' %% of original data')\n",
    "                print('')\n",
    "\n",
    "                if args.downsampling=='random':\n",
    "                    uNet = load_model('./ModelCheckpoint/2D/xf/64/Random/down'+str(down_factors[step])+'/super_res_xf_random_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse(batch_size=1,num_x_points=args.num_x_points),'NMSE': NMSE, 'ncc': ncc, 'ReflectionPadding2D':ReflectionPadding2D})\n",
    "\n",
    "                elif args.downsampling=='regular':\n",
    "                    uNet = load_model('./ModelCheckpoint/2D/xf/64/Regular/down'+str(down_factors[step])+'/super_res_xf_down'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse(batch_size=1,num_x_points=args.num_x_points),'NMSE': NMSE, 'ncc': ncc, 'ReflectionPadding2D':ReflectionPadding2D})\n",
    "                else:\n",
    "                    uNet = load_model('../ModelCheckpoint/super_res_complex_xf_'+str(down_factors[step])+'.h5', \n",
    "                        custom_objects = {'loss': mask_mse(batch_size=1,num_x_points=args.num_x_points),'NMSE': NMSE, 'ncc': ncc, 'ReflectionPadding2D':ReflectionPadding2D})\n",
    "                \n",
    "                uNet.compile(loss=mask_mse(batch_size=1,num_x_points=args.num_x_points), optimizer=opt, metrics=[NMSE, ncc])\n",
    "\n",
    "                score = uNet.evaluate(X_test, Y_test, verbose=1, batch_size=1)\n",
    "                probs = uNet.predict(X_test, verbose=1, batch_size=1)\n",
    "\n",
    "                print(\"Calculating U-net NMSE and NCC for predictions\")\n",
    "\n",
    "                list_metrics_Unet = []\n",
    "                list_plots_Unet = []\n",
    "\n",
    "                for idx in range(len(Y_test)):\n",
    "                    down = X_test[idx][:,:,0]\n",
    "                    ground_truth = Y_test[idx][:,:,0]\n",
    "                    prediction = probs[idx][:,:,0]\n",
    "\n",
    "                    nmse_Unet = nmse(ground_truth,prediction)\n",
    "                    ncc_Unet = NCC(ground_truth,prediction)\n",
    "                    list_metrics_Unet.append((nmse_Unet,ncc_Unet))\n",
    "                    list_plots_Unet.append((ground_truth, down, prediction))\n",
    "\n",
    "                if args.downsampling=='regular':\n",
    "                    with open('./Metrics/2D/xf/64/Paper/Unets/Regular/metrics_2D_Unet_downtrain'+str(down_factors[step])+'_downtest'+str(x_down_factor*100)+'%%data','wb') as output:\n",
    "                        pickle.dump(list_metrics_Unet,output)\n",
    "                else:\n",
    "                    with open('./Metrics/2D/xf/64/Paper/Unets/Random/metrics_2D_Unet_downtrain'+str(down_factors[step])+'_downtest'+str(x_down_factor*100)+'%%data','wb') as output:\n",
    "                        pickle.dump(list_metrics_Unet,output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
